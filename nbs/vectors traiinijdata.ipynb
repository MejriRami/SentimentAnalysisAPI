{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89e444c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2432a73",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "USE_PROJECT_ROOT = True\n",
    "BASE_DIR = pathlib.Path().resolve()\n",
    "if USE_PROJECT_ROOT:\n",
    "    BASE_DIR = BASE_DIR.parent.parent\n",
    "DATASET_DIR = BASE_DIR / \"datasets\"\n",
    "EXPORT_DIR = DATASET_DIR / \"exports\"\n",
    "DATASET_CSV_PATH = EXPORT_DIR / 'spam-dataset.csv'\n",
    "TRAINING_DATA_PATH = EXPORT_DIR / 'spam-training-data.pkl'\n",
    "print(\"BASE_DIR is\", BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79988a0b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "RUN_DATASET_PREPARE = False\n",
    "if RUN_DATASET_PREPARE:\n",
    "    # if active, this will download and prepare the dataset.\n",
    "    SOURCE_NB = pathlib.Path('1 - Prepare the AI Spam Classifier Dataset.ipynb')\n",
    "    if SOURCE_NB.exists():\n",
    "        %run './{SOURCE_NB}'\n",
    "    else:\n",
    "        print(\"Prepare the AI Spam Classifier Dataset.ipynb does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb188453",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if not DATASET_CSV_PATH.exists():\n",
    "    raise Exception(f\"You must download or create the spam-dataset.csv \\n{DATASET_CSV_PATH} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913084dc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(str(DATASET_CSV_PATH))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054d5579",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "texts = df['text'].tolist()\n",
    "labels = df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0808abd7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "labels_legend = {'ham': 0, 'spam': 1}\n",
    "labels_legend_inverted = {f\"{v}\":k for k,v in labels_legend.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab07588",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "labels_as_int =  [labels_legend[str(x)] for x in labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab412ccf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random_idx = random.randint(0, len(texts))\n",
    "print('Random Index', random_idx)\n",
    "\n",
    "assert texts[random_idx] == df.iloc[random_idx].text\n",
    "assert labels[random_idx] == df.iloc[random_idx].label\n",
    "assert labels_legend_inverted[str(labels_as_int[random_idx])] == labels[random_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc64074",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fe9d0c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS=280\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a0018c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c697cbc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "assert len(sequences) == len(texts) == len(labels_as_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814f208e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 280\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b112cb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y = to_categorical(np.asarray(labels_as_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75d3864",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f5f6f6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72974c3a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "training_data = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'max_words': MAX_NUM_WORDS,\n",
    "    'max_sequence': MAX_SEQUENCE_LENGTH,\n",
    "    'legend': labels_legend,\n",
    "    'labels_legend_inverted': labels_legend_inverted,\n",
    "    \"tokenizer\": tokenizer,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9076c4b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "with open(TRAINING_DATA_PATH, 'wb') as f:\n",
    "    pickle.dump(training_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba0af95",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "\n",
    "with open(TRAINING_DATA_PATH, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
