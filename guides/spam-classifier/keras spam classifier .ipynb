!pip install -q pandas tensorflow huggingface_hub

import os, pathlib, json, pickle
import pandas as pd
import numpy as np
from huggingface_hub import login, upload_file
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# === Directories and paths ===
EXPORT_DIR = pathlib.Path('/datasets/exports/')
EXPORT_DIR.mkdir(parents=True, exist_ok=True)
DATASET_CSV_PATH = EXPORT_DIR / 'spam-dataset.csv'
TRAINING_DATA_PATH = EXPORT_DIR / 'spam-training-data.pkl'
MODEL_EXPORT_PATH = EXPORT_DIR / 'spam-model.h5'
TOKENIZER_EXPORT_PATH = EXPORT_DIR / 'spam-classifer-tokenizer.json'
METADATA_EXPORT_PATH = EXPORT_DIR / 'spam-classifer-metadata.json'

# === Download dataset ===
!curl -s "https://raw.githubusercontent.com/codingforentrepreneurs/AI-as-an-API/main/datasets/exports/spam-dataset.csv" -o "$DATASET_CSV_PATH"

# === Load preprocessed vectors (assumes you already created this pickle file) ===
!curl -s "https://huggingface.co/datasets/ramimejri/spam-sms-model/resolve/main/exports/spam-sms/spam-training-data.pkl" -o "$TRAINING_DATA_PATH"

with open(TRAINING_DATA_PATH, 'rb') as f:
    data = pickle.load(f)

X_train = data['X_train']
X_test = data['X_test']
y_train = data['y_train']
y_test = data['y_test']
labels_legend_inverted = data['labels_legend_inverted']
legend = data['legend']
max_sequence = data['max_sequence']
max_words = data['max_words']
tokenizer = data['tokenizer']

# === Train the LSTM model ===
embed_dim = 128
lstm_out = 196

model = Sequential()
model.add(Embedding(max_words, embed_dim, input_length=X_train.shape[1]))
model.add(SpatialDropout1D(0.4))
model.add(LSTM(lstm_out, dropout=0.3, recurrent_dropout=0.3))
model.add(Dense(2, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=32, epochs=5, verbose=1)

# === Export model, tokenizer, metadata ===
model.save(MODEL_EXPORT_PATH)
with open(TOKENIZER_EXPORT_PATH, 'w') as f:
    f.write(tokenizer.to_json())
with open(METADATA_EXPORT_PATH, 'w') as f:
    json.dump({
        "labels_legend_inverted": labels_legend_inverted,
        "legend": legend,
        "max_sequence": max_sequence,
        "max_words": max_words,
    }, f, indent=4)

# === Upload to Hugging Face ===
login("your_huggingface_token_here")  # Replace with your actual token
REPO_ID = "ramimejri/spam-sms-model"  # Change if needed

upload_file(path_or_fileobj=str(MODEL_EXPORT_PATH), path_in_repo=f"exports/spam-sms/{MODEL_EXPORT_PATH.name}", repo_id=REPO_ID, repo_type="dataset")
upload_file(path_or_fileobj=str(TOKENIZER_EXPORT_PATH), path_in_repo=f"exports/spam-sms/{TOKENIZER_EXPORT_PATH.name}", repo_id=REPO_ID, repo_type="dataset")
upload_file(path_or_fileobj=str(METADATA_EXPORT_PATH), path_in_repo=f"exports/spam-sms/{METADATA_EXPORT_PATH.name}", repo_id=REPO_ID, repo_type="dataset")
